{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Based on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1757500069124
        }
      },
      "outputs": [],
      "source": [
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "# Check if given credential can get token successfully.\n",
        "credential.get_token(\"https://management.azure.com/.default\")\n",
        "\n",
        "\n",
        "ml_client = MLClient.from_config(credential=credential)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prep-data.py\n",
        "\n",
        "import argparse\n",
        "import csv\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "def get_data(path):\n",
        "    import os, glob, tempfile, shutil, csv\n",
        "    from pathlib import Path\n",
        "\n",
        "    p = Path(path)\n",
        "    if p.is_dir():\n",
        "        candidates = sorted(p.glob(\"*.csv\"))\n",
        "        if not candidates:\n",
        "            raise FileNotFoundError(f\"No CSV files in: {p}\")\n",
        "        p = candidates[0]\n",
        "\n",
        "    # copy to local temp to avoid mount read(nbytes) issues\n",
        "    fd, tmp = tempfile.mkstemp(suffix=p.suffix or \".csv\"); os.close(fd)\n",
        "    shutil.copyfile(str(p), tmp)\n",
        "\n",
        "    # detect delimiter (fallback to comma)\n",
        "    try:\n",
        "        with open(tmp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            sample = f.read(20000)\n",
        "        sep = csv.Sniffer().sniff(sample, delimiters=[\",\",\";\",\"\\t\",\"|\"]).delimiter\n",
        "    except Exception:\n",
        "        sep = \",\"\n",
        "\n",
        "    # robust read with python engine\n",
        "    try:\n",
        "        df = pd.read_csv(tmp, engine=\"python\", sep=sep)\n",
        "    except UnicodeDecodeError:\n",
        "        df = pd.read_csv(tmp, engine=\"python\", sep=sep, encoding=\"latin-1\")\n",
        "\n",
        "    print(f\"Preparing {len(df)} rows of data\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # simple NA drop; extend with domain-specific cleaning if needed\n",
        "    before = df.shape[0]\n",
        "    df = df.dropna()\n",
        "    print(f\"[DEBUG] clean_data: dropped {before - df.shape[0]} rows with NA\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def normalize_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Handle common column name variants (Pima Indians dataset vs names)\n",
        "    alias_groups = {\n",
        "        \"Pregnancies\": [\"Pregnancies\"],\n",
        "        \"Glucose\": [\"Glucose\", \"PlasmaGlucose\"],\n",
        "        \"BloodPressure\": [\"BloodPressure\", \"DiastolicBloodPressure\"],\n",
        "        \"SkinThickness\": [\"SkinThickness\", \"TricepsThickness\"],\n",
        "        \"Insulin\": [\"Insulin\", \"SerumInsulin\"],\n",
        "        \"BMI\": [\"BMI\"],\n",
        "        \"DiabetesPedigreeFunction\": [\"DiabetesPedigreeFunction\", \"DiabetesPedigree\"],\n",
        "        # Add \"Age\" if you want to scale it too\n",
        "    }\n",
        "\n",
        "    # Pick whichever alias exists in df\n",
        "    cols_to_scale = []\n",
        "    for canonical, aliases in alias_groups.items():\n",
        "        for a in aliases:\n",
        "            if a in df.columns:\n",
        "                cols_to_scale.append(a)\n",
        "                break\n",
        "\n",
        "    if not cols_to_scale:\n",
        "        raise ValueError(\"None of the expected numeric columns were found to scale.\")\n",
        "\n",
        "    # Coerce to numeric\n",
        "    for c in cols_to_scale:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "    # Drop rows that became NaN in any scaled column\n",
        "    before = df.shape[0]\n",
        "    df = df.dropna(subset=cols_to_scale)\n",
        "    print(f\"[DEBUG] normalize_data: coerced to numeric; dropped {before - df.shape[0]} rows with non-numeric values in {cols_to_scale}\")\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
        "    print(f\"[DEBUG] normalized columns: {cols_to_scale}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    df = get_data(args.input_data)\n",
        "    print(f\"Preparing {len(df)} rows of data\")\n",
        "\n",
        "    cleaned = clean_data(df)\n",
        "    normalized = normalize_data(cleaned)\n",
        "\n",
        "    out_dir = Path(args.output_data)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    out_path = out_dir / \"diabetes.csv\"\n",
        "    normalized.to_csv(out_path, index=False)\n",
        "    print(f\"[INFO] wrote: {out_path} rows={len(normalized)}\")\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--input_data\", required=True, type=str)\n",
        "    parser.add_argument(\"--output_data\", required=True, type=str)\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"start main\")\n",
        "    print(\"\\n\\n\" + \"*\" * 60)\n",
        "    args = parse_args()\n",
        "    main(args)\n",
        "    print(\"*\" * 60 + \"\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prep-data.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: prep_data\n",
        "display_name: Prepare training data\n",
        "version: 1\n",
        "type: command\n",
        "inputs:\n",
        "  input_data: \n",
        "    type: uri_file\n",
        "outputs:\n",
        "  output_data:\n",
        "    type: uri_folder\n",
        "code: ./\n",
        "environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
        "command: >-\n",
        "  python prep-data.py --input_data ${{inputs.input_data}} --output_data ${{outputs.output_data}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile train-model.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: train_model\n",
        "display_name: Train a logistic regression model\n",
        "version: 1\n",
        "type: command\n",
        "inputs:\n",
        "  training_data: \n",
        "    type: uri_folder\n",
        "  reg_rate:\n",
        "    type: number\n",
        "    default: 0.01\n",
        "outputs:\n",
        "  model_output:\n",
        "    type: mlflow_model\n",
        "code: ./\n",
        "environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
        "command: >-\n",
        "  python train-model-mlflow.py --training_data ${{inputs.training_data}} --reg_rate ${{inputs.reg_rate}} --model_output ${{outputs.model_output}} "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1757500091939
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import load_component\n",
        "parent_dir = \"\"\n",
        "\n",
        "prep_data = load_component(source=parent_dir + \"./prep-data.yml\")\n",
        "train_logistic_regression = load_component(source=parent_dir + \"./train-model.yml\")\n",
        "\n",
        "print(train_logistic_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "gather": {
          "logged": 1757500096968
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "\n",
        "@pipeline()\n",
        "def diabetes_classification(pipeline_job_input):\n",
        "    clean_data = prep_data(input_data=pipeline_job_input)\n",
        "    \n",
        "    train_model = train_logistic_regression(training_data=clean_data.outputs.output_data)\n",
        "\n",
        "    return {\n",
        "        \"pipeline_job_transformed_data\": clean_data.outputs.output_data,\n",
        "        \"pipeline_job_trained_model\": train_model.outputs.model_output,\n",
        "    }\n",
        "\n",
        "pipeline_job = diabetes_classification(Input(type=AssetTypes.URI_FILE, path=\"azureml:diabetes:1\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1757500099368
        }
      },
      "outputs": [],
      "source": [
        "# change the output mode\n",
        "pipeline_job.outputs.pipeline_job_transformed_data.mode = \"upload\"\n",
        "pipeline_job.outputs.pipeline_job_trained_model.mode = \"upload\"\n",
        "# set pipeline level compute\n",
        "pipeline_job.settings.default_compute = \"ml-compute-ntb\"\n",
        "# set pipeline level datastore\n",
        "pipeline_job.settings.default_datastore = \"workspaceblobstore\"\n",
        "\n",
        "print(pipeline_job)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1757500108617
        }
      },
      "outputs": [],
      "source": [
        "ml_client.create_or_update(pipeline_job, experiment_name=\"diabetes_pipeline\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
